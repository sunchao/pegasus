<?xml version="1.0" encoding="UTF-8"?>

<configuration>

<!--cluster overrides-->
  <property>
    <name>net.topology.script.file.name</name>
    <value>/opt/hadoop/topology.py</value>
  </property>
  <property>
    <name>mapreduce.application.classpath</name>
    <value>$HADOOP_MAPRED_HOME/*,$HADOOP_MAPRED_HOME/lib/*,$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*,$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*,$MR2_CLASSPATH</value>
  </property>

<!-- server setup section -->
  <property>
    <name>hive.metastore.client.socket.timeout</name>
    <value>300</value>
  </property>
  <property>
    <name>hive.metastore.warehouse.dir</name>
    <value>/user/hive/warehouse</value>
  </property>
  <property>
    <name>hive.warehouse.subdir.inherit.perms</name>
    <value>true</value>
  </property>
  <property>
    <name>hive.support.concurrency</name>
    <value>false</value>
  </property>
  <property>
    <name>hive.server2.support.dynamic.service.discovery</name>
    <value>false</value>
  </property>
  <property>
    <name>hive.server2.zookeeper.namespace</name>
    <value>hiveserver2_prod_nosuch</value>
  </property>
  <property>
    <name>hive.zookeeper.client.port</name>
    <value>2181</value>
  </property>
  <property>
    <name>hive.zookeeper.namespace</name>
    <value>hive_zookeeper_namespace_generic</value>
  </property>
  <property>
    <name>hive.cluster.delegation.token.store.class</name>
    <value>org.apache.hadoop.hive.thrift.MemoryTokenStore</value>
  </property>
  <property>
    <name>hive.server2.thrift.min.worker.threads</name>
    <value>5</value>
  </property>
  <property>
    <name>hive.server2.thrift.max.worker.threads</name>
    <value>512</value>
  </property>
  <property>
    <name>hive.server2.thrift.worker.keepalive.time</name>
    <value>60s</value>
  </property>
  <property>
    <name>hive.entity.capture.input.URI</name>
    <value>true</value>
  </property>
  <property>
    <name>hive.server2.enable.doAs</name>
    <value>false</value>
  </property>
  <property>
    <name>hive.metastore.execute.setugi</name>
    <value>true</value>
  </property>
  <property>
    <name>hive.server2.use.SSL</name>
    <value>false</value>
  </property>
  <property>
    <name>datanucleus.autoStartMechanism</name>
    <value>SchemaTable</value>
  </property>
  <property>
    <name>datanucleus.transactionIsolation</name>
    <value>repeatable-read</value>
  </property>
  <property>
    <name>datanucleus.valuegeneration.transactionIsolation</name>
    <value>repeatable-read</value>
  </property>
  <property>
    <name>hive.server2.async.exec.threads</name>
    <value>100</value>
  </property>
  <property>
    <name>hive.server2.async.exec.keepalive.time</name>
    <value>10s</value>
  </property>
  <property>
    <name>hive.server2.async.exec.wait.queue.size</name>
    <value>100</value>
  </property>
  <property>
       <name>hive.server2.max.start.attempts</name>
       <value>1</value>
  </property>
  <property>
    <name>hive.execution.engine</name>
    <value>mr</value>
  </property>
  <property>
    <name>mapreduce.job.queuename</name>
    <value>hive</value>
  </property>
  <property>
    <name>tez.queue.name</name>
    <value>hive</value>
  </property>
 <property>
    <name>hive.server2.logging.operation.log.location</name>
    <value>/var/log/hive/operation_logs</value>
  </property>
  <property>
    <name>hive.server2.metrics.enabled</name>
    <value>true</value>
  </property>
  <property>
    <name>hive.service.metrics.reporter</name>
    <value>JMX</value>
  </property>
  <property>
    <name>hive.skip.hdfs.encryption</name>
    <value>true</value>
  </property>
  <property>
    <name>yarn.app.mapreduce.am.staging-dir</name>
    <value>/tmp</value>
  </property>


<!-- performance section -->

  <property>
    <name>hive.input.format</name>
    <value>com.uber.hoodie.hadoop.hive.HoodieCombineHiveInputFormat</value>
  </property>
<!-- <property>
    <name>hive.tez.input.format</name>
    <value>com.uber.hoodie.hadoop.hive.HoodieCombineHiveInputFormat</value>
  </property>
-->
  <property>
    <name>hive.auto.convert.join</name>
    <value>true</value>
  </property>
  <property>
    <name>hive.auto.convert.join.noconditionaltask.size</name>
    <value>20971520</value>
  </property>
  <property>
    <name>hive.optimize.index.filter</name>
    <value>true</value>
  </property>
  <property>
    <name>hive.optimize.bucketmapjoin.sortedmerge</name>
    <value>false</value>
  </property>
  <property>
    <name>hive.smbjoin.cache.rows</name>
    <value>10000</value>
  </property>
  <property>
    <name>mapred.reduce.tasks</name>
    <value>-1</value>
  </property>
  <property>
    <name>mapred.max.split.size</name>
    <value>536870912</value>
  </property>
  <property>
    <name>hive.exec.reducers.bytes.per.reducer</name>
    <value>536870912</value>
  </property>
  <property>
    <name>hive.exec.copyfile.maxsize</name>
    <value>1073741824</value>
  </property>
  <property>
    <name>hive.exec.reducers.max</name>
    <value>1099</value>
  </property>
  <property>
    <name>hive.vectorized.groupby.checkinterval</name>
    <value>4096</value>
  </property>
  <property>
    <name>hive.vectorized.groupby.flush.percent</name>
    <value>0.1</value>
  </property>
  <property>
    <name>hive.compute.query.using.stats</name>
    <value>false</value>
  </property>
  <property>
    <name>hive.vectorized.execution.enabled</name>
    <value>true</value>
  </property>
  <property>
    <name>hive.vectorized.execution.reduce.enabled</name>
    <value>false</value>
  </property>
  <property>
    <name>hive.merge.mapfiles</name>
    <value>true</value>
  </property>
  <property>
    <name>hive.merge.mapredfiles</name>
    <value>true</value>
  </property>
  <property>
    <name>hive.cbo.enable</name>
    <value>false</value>
  </property>
  <property>
    <name>hive.fetch.task.conversion</name>
    <value>more</value>
  </property>
  <property>
    <name>hive.fetch.task.conversion.threshold</name>
    <value>1073741824</value>
  </property>
  <property>
    <name>hive.limit.pushdown.memory.usage</name>
    <value>0.1</value>
  </property>
  <property>
    <name>hive.merge.smallfiles.avgsize</name>
    <value>134217728</value>
  </property>
  <property>
    <name>hive.merge.size.per.task</name>
    <value>2147483648</value>
  </property>
  <property>
    <name>hive.optimize.reducededuplication</name>
    <value>true</value>
  </property>
  <property>
    <name>hive.optimize.reducededuplication.min.reducer</name>
    <value>4</value>
  </property>
  <property>
    <name>hive.map.aggr</name>
    <value>true</value>
  </property>
  <property>
    <name>hive.map.aggr.hash.percentmemory</name>
    <value>0.5</value>
  </property>
  <property>
    <name>hive.optimize.sort.dynamic.partition</name>
    <value>false</value>
  </property>
  <property>
    <name>hive.stats.fetch.column.stats</name>
    <value>false</value>
  </property>
  <property>
    <name>hive.stats.collect.scancols</name>
    <value>true</value>
  </property>
  <property>
    <name>hive.groupby.orderby.position.alias</name>
    <value>true</value>
  </property>
  <property>
    <name>mapreduce.job.input.size.limit</name>
    <value>42000000000000</value>
  </property>
  <property>
    <name>mapreduce.job.reduce.slowstart.completedmaps</name>
    <value>0.99</value>
  </property>
  <property>
    <name>hive.mapred.supports.subdirectories</name>
    <value>true</value>
  </property>
  <property>
    <name>hive.mapred.reduce.tasks.speculative.execution</name>
    <value>false</value>
  </property>
  <property>
    <name>mapred.dfsclient.parallelism.max</name>
    <value>15</value>
  </property>
  <property>
    <name>hive.prune.nested.col</name>
    <value>true</value>
  </property>
  <property>
    <name>mapreduce.input.fileinputformat.input.dir.recursive</name>
    <value>true</value>
  </property>


<!-- ops sections -->
  <property>
    <name>hive.strict.checks.large.query</name>
    <value>true</value>
  </property>
  <property>
    <name>hive.strict.checks.cartesian.product</name>
    <value>false</value>
  </property>
  <property>
    <name>hive.strict.checks.type.safety</name>
    <value>false</value>
  </property>
  <property>
    <name>hive.server2.idle.session.check.operation</name>
    <value>true</value>
  </property>
  <property>
    <name>hive.server2.idle.session.timeout</name>
    <value>7200000</value>
  </property>
  <property>
    <name>hive.server2.idle.operation.timeout</name>
    <value>1800000</value>
  </property>
  <property>
    <name>hive.server2.session.check.interval</name>
    <value>30000</value>
  </property>
  <property>
    <name>hive.query.timeout.seconds</name>
    <value>43200</value>
  </property>
  <property>
    <name>hive.security.command.whitelist</name>
    <value>set,reset</value>
  </property>
<!--  <property>
    <name>hive.compile.timeout</name>
    <value>5m</value>
  </property>
-->
  <property>
    <name>hive.conf.restricted.list</name>
    <value>hive.security.authenticator.manager,hive.security.authorization.manager,hive.users.in.admin.role,hive.semantic.analyzer.hook,hive.exec.pre.hooks,hive.exec.scratchdir,hive.exec.local.scratchdir,hive.metastore.uris,javax.jdo.option.ConnectionURL,hadoop.bin.path,hive.session.id,hive.aux.jars.path,hive.stats.dbconnectionstring,hive.scratch.dir.permission,hive.security.command.whitelist,hive.security.authorization.task.factory,hive.entity.capture.transform,hive.reloadable.aux.jars.path,hive.access.conf.url,hive.sentry.conf.url,hive.access.subject.name,hive.sentry.subject.name,hive.sentry.active.role.set</value>
 </property>
  <property>
    <name>hive.exec.max.created.files</name>
    <value>100000</value>
  </property>
  <property>
    <name>mapreduce.job.max.map</name>
    <value>80000</value>
  </property>

  <!-- viewfs section -->
  <property>
    <name>hive.exec.scratchdir</name>
    <value>viewfs://ns-default/tmp/hive_scratchdir</value>
  </property>

  <property>
    <name>hive.exec.tmptable.dir</name>
    <value>/tmp/hive_tmptables</value>
  </property>

  <property>
    <name>hive.exec.stagingdir</name>
    <value>/tmp/hive_stagingdir/${user.name}</value>
  </property>

<!-- hooks section -->

  <property>
    <name>hive.server2.session.hook</name>
    <value>org.apache.sentry.binding.hive.HiveAuthzBindingSessionHook</value>
  </property>
  <property>
    <name>hive.security.authorization.manager</name>
    <value>org.apache.sentry.binding.hive.HiveAuthzBindingSessionHook$SentryHiveAuthorizerFactory</value>
  </property>
  <property>
    <name>hive.clustername</name>
    <value>defaultClusterName</value>
  </property>
  <property>
    <name>hive.exec.failure.hooks</name>
    <value>com.uber.data.hook.HiveQueryInfo</value>
  </property>
  <property>
    <name>hive.exec.pre.hooks</name>
    <value>com.uber.data.hook.HiveQueryInfo</value>
  </property>
  <property>
    <name>hive.exec.post.hooks</name>
    <value>com.uber.data.hook.HiveQueryInfo,com.uber.data.hook.AddTableOwnerHook</value>
  </property>
  <property>
    <name>hive.query.lifetime.hooks</name>
    <value>com.uber.data.geospatial.HiveGeoIndexHook</value>
  </property>

<!-- tez section -->
  <property>
    <name>tez.am.shuffle-vertex-manager.min-src-fraction</name>
    <value>1</value>
  </property>
  <property>
    <name>tez.am.shuffle-vertex-manager.max-src-fraction</name>
    <value>1</value>
  </property>
  <property>
    <name>hive.merge.tezfiles</name>
    <value>true</value>
  </property>
  <property>
    <name>hive.tez.dynamic.partition.pruning.max.data.size</name>
    <value>104857600000</value>
  </property>
  <property>
    <name>hive.tez.dynamic.partition.pruning.max.event.size</name>
    <value>104857600000</value>
  </property>


<!-- spark section -->
  <property>
    <name>spark.master</name>
    <value>yarn-cluster</value>
  </property>
  <property>
    <name>spark.dynamicAllocation.enabled</name>
    <value>true</value>
  </property>
  <property>
    <name>spark.shuffle.service.enabled</name>
    <value>true</value>
  </property>
  <property>
    <name>hive.merge.sparkfiles</name>
    <value>true</value>
  </property>
  <property>
    <name>spark.executor.memory</name>
    <value>7168m</value>
  </property>
  <property>
    <name>spark.driver.memory</name>
    <value>6582m</value>
  </property>
  <property>
    <name>spark.driver.cores</name>
    <value>4</value>
  </property>
  <property>
    <name>spark.executor.cores</name>
    <value>4</value>
  </property>
  <property>
    <name>spark.yarn.driver.memoryOverhead</name>
    <value>1152</value>
  </property>
  <property>
    <name>spark.yarn.executor.memoryOverhead</name>
    <value>3072</value>
  </property>
  <property>
    <name>spark.shuffle.manager</name>
    <value>tungsten-sort</value>
  </property>
  <property>
    <name>spark.dynamicAllocation.initialExecutors</name>
    <value>5</value>
  </property>
  <property>
    <name>spark.dynamicAllocation.minExecutors</name>
    <value>0</value>
  </property>
  <property>
    <name>spark.dynamicAllocation.maxExecutors</name>
    <value>1000</value>
  </property>
  <property>
    <name>spark.locality.wait</name>
    <value>0s</value>
  </property>
  <property>
    <name>spark.network.timeout</name>
    <value>800</value>
  </property>
  <property>
    <name>spark.executor.heartbeatInterval</name>
    <value>60s</value>
  </property>
  <property>
    <name>spark.shuffle.registration.timeout</name>
    <value>60000</value>
  </property>
  <property>
    <name>spark.shuffle.registration.maxAttempts</name>
    <value>5</value>
  </property>
  <property>
    <name>spark.shuffle.registration.attempt.interval</name>
    <value>30</value>
  </property>
  <property>
    <name>hive.spark.client.server.connect.timeout</name>
    <value>1800000ms</value>
  </property>
  <property>
    <name>spark.eventLog.enabled</name>
    <value>true</value>
  </property>
  <property>
    <name>spark.eventLog.dir</name>
    <value>viewfs://ns-default/tmp/hive-on-spark-logs/applicationHistory</value>
  </property>
  <property>
    <name>spark.yarn.historyServer.address</name>
    <value>hadoopservice24-sjc1.prod.uber.internal:8629</value>
  </property>
  <property>
    <name>hive.spark.exec.inplace.progress</name>
    <value>true</value>
  </property>
  <property>
    <name>spark.driver.extraJavaOptions</name>
    <value>-javaagent:uber-java-agent-1.0.9-jar-with-dependencies.jar=tag=hos_driver,cluster=hive -Djava.net.preferIPv4Stack=true</value>
  </property>
  <property>
    <name>spark.executor.extraJavaOptions</name>
    <value>-javaagent:uber-java-agent-1.0.9-jar-with-dependencies.jar=tag=hos_executor,cluster=hive -Djava.net.preferIPv4Stack=true</value>
  </property>
  <property>
    <name>hive.spark.use.op.stats</name>
    <value>false</value>
  </property>
  <property>
    <name>hive.spark.use.file.size.for.mapjoin</name>
    <value>true</value>
  </property>
  <property>
    <name>spark.scheduler.listenerbus.eventqueue.size</name>
    <value>80000</value>
  </property>
  <property>
    <name>hive.spark.client.connect.timeout</name>
    <value>5000</value>
  </property>
  <property>
    <name>hive.spark.job.max.tasks</name>
    <value>200000</value>
  </property>
  <property>
    <name>hive.spark.stage.max.tasks</name>
    <value>80000</value>
  </property>
  <property>
    <name>spark.dynamicAllocation.executorIdleTimeout</name>
    <value>5</value>
  </property>
  <property>
    <name>spark.dynamicAllocation.tasksPerExecutorSlot</name>
    <value>2</value>
  </property>


<!--files on local installation -->

  <property>
    <name>hive.aux.jars.path</name>
     <value>file://${hive2_udeploy_home}/lib/hoodie-mr-0.2.9.jar,file://${hive2_udeploy_home}/lib/json-serde-1.3.7-jar-with-dependencies.jar,file://${hive2_udeploy_home}/lib/elasticsearch-hadoop-5.6.3.jar</value>
  </property>

 <property>
   <name>hive.sentry.conf.url</name>
   <value>file://${hive2_udeploy_home}/../uber_hive_conf/hive_conf_basic/sentry-site.xml</value>
 </property>


<!--uber java agent -->
  <property>
    <name>spark.jars</name>
    <value>${hive2_udeploy_home}/lib/antlr-runtime-3.4.jar,${hive2_udeploy_home}/lib/antlr-2.7.7.jar,hdfs:///lib/packages/uber_java_agent/uber-java-agent-1.0.9-jar-with-dependencies.jar,hdfs:///lib/packages/uber_java_agent/uber-spark-plugin-0.0.11-hive.jar</value>
  </property>

  <property>
    <name>spark.extraListeners</name>
    <value>com.uber.sparkplugin.HeatpipeSparkListener</value>
  </property>

  <property>
    <name>mapreduce.input.fileinputformat.list-status.num-threads</name>
    <value>4</value>
  </property>

</configuration>
