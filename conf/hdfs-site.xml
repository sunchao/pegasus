<?xml version="1.0" encoding="UTF-8"?>
<configuration>
  <property>
    <name>hadoop.caller.context.enabled</name>
    <value>true</value>
  </property>
  <property>
    <name>dfs.nameservices</name>
    <value>ns-neon-prod-dca1</value>
  </property>
  <property>
    <name>dfs.client.failover.proxy.provider.ns-neon-prod-dca1</name>
    <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
  </property>
  <property>
    <name>dfs.ha.automatic-failover.enabled.ns-neon-prod-dca1</name>
    <value>true</value>
  </property>
  <!-- T2282075 Increase zk session timout -->
  <property>
    <name>ha.zookeeper.session-timeout.ms</name>
    <value>12000</value>
  </property>
  <property>
    <name>ha.zookeeper.quorum</name>
    <value>hadoopzk01-dca1.prod.uber.internal:2181,hadoopzk02-dca1.prod.uber.internal:2181,hadoopzk03-dca1.prod.uber.internal:2181</value>
  </property>
  <property>
    <name>dfs.ha.namenodes.ns-neon-prod-dca1</name>
    <value>nn1,nn2</value>
  </property>
  <property>
    <name>dfs.namenode.rpc-address.ns-neon-prod-dca1.nn1</name>
    <value>hadoopneonnamenode01-dca1.prod.uber.internal:8020</value>
  </property>
  <property>
    <name>dfs.namenode.servicerpc-address.ns-neon-prod-dca1.nn1</name>
    <value>hadoopneonnamenode01-dca1.prod.uber.internal:8022</value>
  </property>
  <property>
    <name>dfs.namenode.http-address.ns-neon-prod-dca1.nn1</name>
    <value>hadoopneonnamenode01-dca1.prod.uber.internal:50070</value>
  </property>
  <property>
    <name>dfs.namenode.https-address.ns-neon-prod-dca1.nn1</name>
    <value>hadoopneonnamenode01-dca1.prod.uber.internal:50470</value>
  </property>
  <property>
    <name>dfs.namenode.rpc-address.ns-neon-prod-dca1.nn2</name>
    <value>hadoopneonnamenode02-dca1.prod.uber.internal:8020</value>
  </property>
  <property>
    <name>dfs.namenode.servicerpc-address.ns-neon-prod-dca1.nn2</name>
    <value>hadoopneonnamenode02-dca1.prod.uber.internal:8022</value>
  </property>
  <property>
    <name>dfs.namenode.http-address.ns-neon-prod-dca1.nn2</name>
    <value>hadoopneonnamenode02-dca1.prod.uber.internal:50070</value>
  </property>
  <property>
    <name>dfs.namenode.https-address.ns-neon-prod-dca1.nn2</name>
    <value>hadoopneonnamenode02-dca1.prod.uber.internal:50470</value>
  </property>
  <property>
    <name>dfs.namenode.http-bind-host</name>
    <value>0.0.0.0</value>
  </property>
  <property>
    <name>dfs.namenode.https-bind-host</name>
    <value>0.0.0.0</value>
  </property>
  <property>
    <name>dfs.namenode.shared.edits.dir</name>
    <value>qjournal://hadoopneonnamenode01-dca1.prod.uber.internal:8485;hadoopneonnamenode02-dca1.prod.uber.internal:8485;hadoopneonqjm01-dca1.prod.uber.internal:8485/ns-neon-prod-dca1</value>
  </property>
  <property>
    <name>dfs.journalnode.edits.dir</name>
    <value>/data2/dfs/jn</value>
  </property>
  <property>
    <name>dfs.permissions.superusergroup</name>
    <value>supergroup</value>
  </property>
  <property>
    <name>dfs.replication</name>
    <value>3</value>
  </property>
  <property>
    <name>dfs.namenode.replication.min</name>
    <value>1</value>
  </property>
  <property>
    <name>dfs.replication.max</name>
    <value>512</value>
  </property>
  <property>
    <name>dfs.blocksize</name>
    <value>536870912</value>
  </property>
  <property>
    <name>dfs.namenode.handler.count</name>
    <value>256</value>
  </property>
  <property>
    <name>dfs.namenode.service.handler.count</name>
    <value>256</value>
  </property>
   <property>
    <name>dfs.namenode.checkpoint.txns</name>
    <value>12000000</value>
  </property>
   <property>
    <name>dfs.namenode.edit.log.autoroll.multiplier.threshold</name>
    <value>0.01</value>
  </property>
   <property>
    <name>dfs.namenode.edit.log.autoroll.check.interval.ms</name>
    <value>150000</value>
  </property>
  <property>
    <name>dfs.namenode.checkpoint.period</name>
    <value>10800</value>
  </property>
  <property>
    <name>dfs.webhdfs.enabled</name>
    <value>true</value>
  </property>
  <property>
    <name>dfs.permissions</name>
    <value>true</value>
  </property>
  <property>
    <name>dfs.client.use.datanode.hostname</name>
    <value>false</value>
  </property>
  <property>
    <name>fs.permissions.umask-mode</name>
    <value>022</value>
  </property>
  <property>
    <name>dfs.encrypt.data.transfer</name>
    <value>false</value>
  </property>
  <property>
    <name>dfs.encrypt.data.transfer.algorithm</name>
    <value>rc4</value>
  </property>
  <property>
    <name>dfs.datanode.address</name>
    <value>0.0.0.0:50010</value>
  </property>
  <property>
    <name>dfs.datanode.ipc.address</name>
    <value>0.0.0.0:50020</value>
  </property>
  <property>
    <name>dfs.datanode.http.address</name>
    <value>0.0.0.0:50075</value>
  </property>
  <property>
    <name>dfs.datanode.https.address</name>
    <value>0.0.0.0:50475</value>
  </property>
  <property>
    <name>dfs.datanode.data.dir.perm</name>
    <value>755</value>
  </property>
  <property>
    <name>dfs.datanode.max.transfer.threads</name>
    <value>4096</value>
  </property>
  <property>
    <name>dfs.datanode.du.reserved</name>
    <value>10737418240</value>
  </property>
  <property>
    <name>dfs.datanode.failed.volumes.tolerated</name>
    <value>3</value>
  </property>
  <property>
    <name>dfs.datanode.balance.bandwidthPerSec</name>
    <value>209715200</value>
  </property>
  <property>
    <name>dfs.datanode.plugins</name>
    <value></value>
  </property>
  <property>
    <name>dfs.datanode.data.dir</name>
    <value>REPLACE_WITH_DFS_DIR</value>
  </property>
  <property>
    <name>dfs.block.local-path-access.user</name>
    <value>hbase,mapred,root</value>
  </property>
  <property>
    <name>dfs.client.read.shortcircuit</name>
    <value>true</value>
  </property>
  <property>
    <name>dfs.domain.socket.path</name>
    <value>/var/run/hdfs-sockets/dn</value>
  </property>
  <property>
    <name>dfs.client.read.shortcircuit.skip.checksum</name>
    <value>false</value>
  </property>
  <property>
    <name>dfs.client.domain.socket.data.traffic</name>
    <value>false</value>
  </property>
  <property>
    <name>dfs.datanode.hdfs-blocks-metadata.enabled</name>
    <value>true</value>
  </property>
  <property>
    <name>dfs.thrift.threads.max</name>
    <value>20</value>
  </property>
  <property>
    <name>dfs.thrift.threads.min</name>
    <value>10</value>
  </property>
  <property>
    <name>dfs.thrift.timeout</name>
    <value>60</value>
  </property>
  <property>
    <name>dfs.datanode.drop.cache.behind.writes</name>
    <value>false</value>
  </property>
  <property>
    <name>dfs.datanode.drop.cache.behind.reads</name>
    <value>false</value>
  </property>
  <property>
    <name>dfs.datanode.sync.behind.writes</name>
    <value>false</value>
  </property>
  <property>
    <name>dfs.datanode.readahead.bytes</name>
    <value>4194304</value>
  </property>
  <property>
    <name>dfs.datanode.use.datanode.hostname</name>
    <value>false</value>
  </property>
  <property>
    <name>dfs.datanode.fsdataset.volume.choosing.policy</name>
    <value>org.apache.hadoop.hdfs.server.datanode.fsdataset.RoundRobinVolumeChoosingPolicy</value>
  </property>
  <property>
    <name>dfs.datanode.max.locked.memory</name>
    <value>4294967296</value>
  </property>
  <property>
    <name>dfs.heartbeat.interval</name>
    <value>30</value>
  </property>
  <property>
    <name>dfs.hosts</name>
    <value>/opt/hdfs/hosts/include</value>
  </property>
  <property>
    <name>dfs.hosts.exclude</name>
    <value>/opt/hdfs/hosts/exclude</value>
  </property>
  <property>
    <name>dfs.namenode.audit.log.async</name>
    <value>true</value>
  </property>
  <property>
    <name>dfs.namenode.edits.asynclogging</name>
    <value>false</value>
  </property>
  <property>
    <name>dfs.namenode.fslock.fair</name>
    <value>false</value>
  </property>
  <property>
    <name>dfs.namenode.name.dir</name>
    <value>file:///data1/dfs/name-data</value>
  </property>
  <property>
    <name>dfs.namenode.accesstime.precision</name>
    <value>0</value>
  </property>
  <property>
    <name>dfs.block.access.token.enable</name>
    <value>true</value>
  </property>

  <!-- Fencing -->
  <property>
    <name>dfs.ha.fencing.methods</name>
    <value>shell(/bin/true)</value>
  </property>

  <!-- Keytabs and principles -->
  <property>
    <name>dfs.journalnode.keytab.file</name>
    <value>/opt/hadoop/keytabs/${user.name}.keytab</value>
  </property>
  <property>
    <name>dfs.journalnode.kerberos.principal</name>
    <value>hdfs/_HOST@DATASRE.PROD.UBER.INTERNAL</value>
  </property>
  <property>
    <name>dfs.journalnode.kerberos.internal.spnego.principal</name>
    <value>HTTP/_HOST@DATASRE.PROD.UBER.INTERNAL</value>
  </property>
  <property>
    <name>dfs.namenode.kerberos.principal</name>
    <value>hdfs/_HOST@DATASRE.PROD.UBER.INTERNAL</value>
  </property>
  <property>
    <name>dfs.namenode.kerberos.internal.spnego.principal</name>
    <value>HTTP/_HOST@DATASRE.PROD.UBER.INTERNAL</value>
  </property>
  <property>
    <name>dfs.secondary.namenode.kerberos.principal</name>
    <value>hdfs/_HOST@DATASRE.PROD.UBER.INTERNAL</value>
  </property>
  <property>
    <name>dfs.secondary.namenode.kerberos.internal.spnego.principal</name>
    <value>HTTP/_HOST@DATASRE.PROD.UBER.INTERNAL</value>
  </property>
  <property>
    <name>dfs.datanode.kerberos.principal</name>
    <value>hdfs/_HOST@DATASRE.PROD.UBER.INTERNAL</value>
  </property>
  <property>
    <name>dfs.namenode.keytab.file</name>
    <value>/opt/hadoop/keytabs/${user.name}.keytab</value>
  </property>
  <property>
    <name>dfs.secondary.namenode.keytab.file</name>
    <value>/opt/hadoop/keytabs/${user.name}.keytab</value>
  </property>
  <property>
    <name>dfs.web.authentication.kerberos.keytab</name>
    <value>/opt/hadoop/keytabs/${user.name}.keytab</value>
  </property>
  <property>
    <name>dfs.web.authentication.kerberos.principal</name>
    <value>HTTP/_HOST@DATASRE.PROD.UBER.INTERNAL</value>
  </property>
  <property>
    <name>dfs.datanode.keytab.file</name>
    <value>/opt/hadoop/keytabs/${user.name}.keytab</value>
  </property>
  <!-- HTTPS configuration -->
  <property>
    <name>dfs.http.policy</name>
    <value>HTTP_AND_HTTPS</value>
  </property>

  <!-- Enable legacy fsimage T711307 -->
  <property>
    <name>dfs.namenode.legacy-oiv-image.dir</name>
    <value>/data3/dfs/legacy</value>
  </property>

  <!-- Increase block report interval to 12 hours T846621 -->
  <property>
    <name>dfs.blockreport.intervalMsec</name>
    <value>43200000</value>
  </property>

  <!-- Disable cache report T939969 -->
  <property>
    <name>dfs.cachereport.intervalMsec</name>
    <value>9223372036854775807</value>
  </property>

  <!-- Disable getFileInfo audit log T1034127 -->
  <!--<property>
    <name>dfs.namenode.audit.loggers</name>
    <value>org.apache.hadoop.hdfs.server.namenode.FilteredAuditLogger</value>
  </property>-->

  <!-- T1125631 Expose more metrics for dev cluster -->

  <!-- Add RPC lock time to audit log -->
  <property>
    <name>dfs.namenode.audit.log.lock.time</name>
    <value>true</value>
  </property>

  <!-- Add RPC queue time and processing time to audit log -->
  <property>
    <name>dfs.namenode.audit.log.rpc.time</name>
    <value>false</value>
  </property>

  <!-- Enable NameNode lock metrics -->
  <property>
    <name>dfs.namenode.lock.detailed-metrics.enabled</name>
    <value>true</value>
  </property>

  <!-- Enable in-progress edit log tailing for observer reads -->
  <property>
    <name>dfs.ha.tail-edits.in-progress</name>
    <value>true</value>
  </property>

  <property>
    <name>dfs.ha.observer.tail-edits.ms.period</name>
    <value>10000</value>
  </property>

  <!-- T1318825 Enable available space block placement policy -->
  <property>
    <name>dfs.block.replicator.classname</name>
    <value>org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceBlockPlacementPolicy</value>
  </property>
  <!-- Here 0.7 means the DataNode with higher available space ratio has 70% probability to be chosen -->
  <property>
    <name>dfs.namenode.available-space-block-placement-policy.balanced-space-preference-fraction</name>
    <value>0.7</value>
  </property>

  <!-- increase replication/decomission speed -->
  <property>
    <name>dfs.namenode.replication.work.multiplier.per.iteration</name>
    <value>32</value>
  </property>
  <property>
    <name>dfs.namenode.replication.max-streams</name>
    <value>32</value>
  </property>
  <property>
    <name>dfs.namenode.replication.max-streams-hard-limit</name>
    <value>64</value>
  </property>

  <!-- T2607331 Increase HDFS NN checkpoint retention period -->
  <property>
    <name>dfs.namenode.num.extra.edits.retained</name>
    <value>12000000</value>
  </property>
  <property>
    <name>dfs.namenode.num.checkpoints.retained</name>
    <value>4</value>
  </property>

  <property>
    <name>uber.conf.package.name</name>
    <value>REPLACE_WITH_PACKAGE_NAME</value>
  </property>
</configuration>
